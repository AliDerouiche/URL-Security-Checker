{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f627db0e-199d-4289-b16f-ca7ebb73e6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1 shape: (651191, 2)\n",
      "Dataset 2 shape: (450176, 4)\n",
      "\n",
      "✅ Dataset merged successfully\n",
      "Final shape: (203460, 2)\n",
      "type\n",
      "benign        138091\n",
      "phishing       55163\n",
      "malware         8083\n",
      "defacement      2123\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# =========================================================\n",
    "# 1. LOAD DATASETS\n",
    "# =========================================================\n",
    "df1 = pd.read_csv(\"malicious_phish.csv\")   # url, type\n",
    "df2 = pd.read_csv(\"urldata.csv\")           # id, url, label, result\n",
    "\n",
    "print(\"Dataset 1 shape:\", df1.shape)\n",
    "print(\"Dataset 2 shape:\", df2.shape)\n",
    "\n",
    "# =========================================================\n",
    "# 2. CLEAN DATASET 2\n",
    "# =========================================================\n",
    "\n",
    "# Keep only useful columns\n",
    "df2 = df2[[\"url\", \"label\"]]\n",
    "\n",
    "# Rename label -> type\n",
    "df2.rename(columns={\"label\": \"type\"}, inplace=True)\n",
    "\n",
    "# =========================================================\n",
    "# 3. NORMALIZE URL (SAME FOR BOTH)\n",
    "# =========================================================\n",
    "def normalize_url(url):\n",
    "    url = str(url).lower()\n",
    "    url = re.sub(r\"https?://\", \"\", url)\n",
    "    url = re.sub(r\"www\\.\", \"\", url)\n",
    "    url = url.split(\"/\")[0]  # domain only\n",
    "    return url\n",
    "\n",
    "df1[\"url\"] = df1[\"url\"].apply(normalize_url)\n",
    "df2[\"url\"] = df2[\"url\"].apply(normalize_url)\n",
    "\n",
    "# =========================================================\n",
    "# 4. NORMALIZE LABELS\n",
    "# =========================================================\n",
    "VALID_LABELS = [\"benign\", \"phishing\", \"malware\", \"defacement\"]\n",
    "\n",
    "df1[\"type\"] = df1[\"type\"].str.lower()\n",
    "df2[\"type\"] = df2[\"type\"].str.lower()\n",
    "\n",
    "df1 = df1[df1[\"type\"].isin(VALID_LABELS)]\n",
    "df2 = df2[df2[\"type\"].isin(VALID_LABELS)]\n",
    "\n",
    "# =========================================================\n",
    "# 5. MERGE DATASETS\n",
    "# =========================================================\n",
    "df_final = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Remove duplicates\n",
    "df_final.drop_duplicates(subset=[\"url\", \"type\"], inplace=True)\n",
    "\n",
    "# Shuffle\n",
    "df_final = df_final.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# =========================================================\n",
    "# 6. SAVE FINAL DATASET\n",
    "# =========================================================\n",
    "df_final.to_csv(\"merged_urls.csv\", index=False)\n",
    "\n",
    "print(\"\\n✅ Dataset merged successfully\")\n",
    "print(\"Final shape:\", df_final.shape)\n",
    "print(df_final[\"type\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16e8f5ad-9df8-471a-b289-3a288334ceef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (204009, 2)\n",
      "\n",
      "Label distribution:\n",
      " type\n",
      "benign        138610\n",
      "phishing       55163\n",
      "malware         8113\n",
      "defacement      2123\n",
      "Name: count, dtype: int64\n",
      "Training model...\n",
      "\n",
      "=== Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign       0.85      0.74      0.79     27674\n",
      "  defacement       0.07      0.27      0.10       424\n",
      "     malware       0.73      0.77      0.75      1623\n",
      "    phishing       0.51      0.62      0.56     11033\n",
      "\n",
      "    accuracy                           0.70     40754\n",
      "   macro avg       0.54      0.60      0.55     40754\n",
      "weighted avg       0.75      0.70      0.72     40754\n",
      "\n",
      "\n",
      "=== Confusion Matrix ===\n",
      "[[20428   904   264  6078]\n",
      " [  118   113     7   186]\n",
      " [  131    44  1250   198]\n",
      " [ 3381   669   193  6790]]\n",
      "\n",
      "=== Sanity Check ===\n",
      "youtube.com                         -> malware    (41.54%)\n",
      "google.com                          -> benign     (82.10%)\n",
      "facebook.com                        -> phishing   (69.03%)\n",
      "\n",
      "Model saved as url_security_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# =========================================================\n",
    "# 1. LOAD DATASET\n",
    "# =========================================================\n",
    "df = pd.read_csv(\"merged_urls.csv\")  # url,type\n",
    "\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nLabel distribution:\\n\", df['type'].value_counts())\n",
    "\n",
    "# =========================================================\n",
    "# 2. CLEAN + NORMALIZE URL\n",
    "# =========================================================\n",
    "def normalize_url(url):\n",
    "    url = str(url).lower()\n",
    "    url = re.sub(r\"https?://\", \"\", url)  # remove http(s)\n",
    "    url = re.sub(r\"www\\.\", \"\", url)      # remove www\n",
    "    url = url.split(\"/\")[0]              # keep only domain\n",
    "    return url\n",
    "\n",
    "df[\"url\"] = df[\"url\"].apply(normalize_url)\n",
    "\n",
    "# Remove duplicates\n",
    "df.drop_duplicates(subset=[\"url\", \"type\"], inplace=True)\n",
    "\n",
    "# =========================================================\n",
    "# 3. TARGET & FEATURES\n",
    "# =========================================================\n",
    "X = df[\"url\"]\n",
    "y = df[\"type\"]\n",
    "\n",
    "# =========================================================\n",
    "# 4. TRAIN / TEST SPLIT\n",
    "# =========================================================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# =========================================================\n",
    "# 5. PIPELINE\n",
    "# =========================================================\n",
    "pipeline = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        analyzer=\"char\",\n",
    "        ngram_range=(3, 5),\n",
    "        min_df=3,\n",
    "        max_features=60000\n",
    "    )),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        class_weight=\"balanced\",\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# =========================================================\n",
    "# 6. TRAIN MODEL\n",
    "# =========================================================\n",
    "print(\"Training model...\")\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# =========================================================\n",
    "# 7. EVALUATION\n",
    "# =========================================================\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(\"\\n=== Classification Report ===\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\n=== Confusion Matrix ===\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# =========================================================\n",
    "# 8. SANITY CHECK ON TOP DOMAINS\n",
    "# =========================================================\n",
    "test_urls = [\n",
    "    \"youtube.com\",\n",
    "    \"google.com\",\n",
    "    \"facebook.com\",\n",
    "]\n",
    "\n",
    "print(\"\\n=== Sanity Check ===\")\n",
    "for u in test_urls:\n",
    "    u_clean = normalize_url(u)\n",
    "    pred = pipeline.predict([u_clean])[0]\n",
    "    proba = pipeline.predict_proba([u_clean]).max()\n",
    "    print(f\"{u:35} -> {pred:10} ({proba*100:.2f}%)\")\n",
    "\n",
    "# =========================================================\n",
    "# 9. SAVE MODEL\n",
    "# =========================================================\n",
    "joblib.dump(pipeline, \"url_security_model.pkl\",)\n",
    "print(\"\\nModel saved as url_security_model.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28999d7-6803-4234-8e1c-d953fbc1bc97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
